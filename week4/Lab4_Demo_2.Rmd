---
title: "Lab 4_Demo"
author: "Mateo Robbins"
date: "2024-04-22"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
```

This data set includes more possible predictors than the text alone, but for this model we will only use the text variable
```{r data}

urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))
```

Now we'll split our data into training and test portions

```{r split-data}
set.seed(1234)

incidents2class <- incidents_df %>%
  mutate(fatal = factor(if_else(
                        is.na(Deadly),
                        "non-fatal", "fatal")))


incidents_split <- initial_split(incidents2class, strata = fatal)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```

We use recipe() to specify the predictor and outcome variables and the data.

```{r recipe}
incidents_rec <- recipe(fatal ~ Text, data = incidents_train)

```

Next we add some familiar pre-processing steps on our Text variable: tokenize to word level, filter to the most common words, and calculate tf-idf.

```{r pre-process}
recipe <- incidents_rec %>%
  step_tokenize(Text) %>%
  step_tokenfilter(Text, max_tokens = 1000) %>% #1000
  step_tfidf(Text)
```

Create  tidymodels workflow to combine the modeling components

```{r workflow}
incidents_wf <- workflow() %>%
  add_recipe(recipe)
```

```{r nb-spec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec

```

Now we are ready to add our model to the workflow and fit it to the training data

```{r fit-model}
nb_fit <- incidents_wf %>%
  add_model(nb_spec) %>%
  fit(data = incidents_train)
```
Next up is model evaluation. We'll stretch our training data further and use resampling to evaluate our naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.

```{r cv_folds}
set.seed(999)
incidents_folds <- vfold_cv(incidents_train)

incidents_folds

```

```{r nb-workflow}
  nb_wf <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(nb_spec)
  
  nb_wf
```

To estimate its performance, we fit the model to each of the resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r fit-resamples}
nb_rs <- fit_resamples(
  nb_wf, 
  incidents_folds, 
  control = control_resamples(save_pred = T)
)
```

Extract the relevant information using collect_metrics() and collect_predictions() and examine the performance metrics.

```{r performance}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics
```

We'll use two performance metrics: accuracy and ROC AUC.
Accuracy is the proportion of the data that is predicted correctly. 
The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.

```{r performance-plot}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = fatal, .pred_fatal) %>%
  autoplot() +
  labs(title = "ROC curve for Climbing Incident Reports")
```

Another model method involves the confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.

```{r confusion-matrix}
conf_mat_resampled(nb_rs, tidy = F) %>%
  autoplot(type = "heatmap")
```

Let's move up to a more sophisticated model. Recall that lasso classification model uses regularization on regression to help us choose a simpler, more generalizable model.  Variable selection helps us identify which features to include in our model.

Lasso classification learns how much of a penalty to put on features to reduce the high-dimensional space of original possible variables (tokens) for the final model.

```{r lasso-specification}
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")
```

```{r lasso-workflow}
lasso_wf  <- workflow() %>% 
  add_model(lasso_spec) %>% 
  add_recipe(recipe)
```

```{r fit-resamples-lasso}
set.seed(123)

lasso_rs <- fit_resamples(
  lasso_wf,
  incidents_folds,
  control = control_resamples(save_pred = T)
)
```

```{r fit-resample-lasso}
lasso_rs_metrics <- collect_metrics(lasso_rs)
lasso_rs_predictions <- collect_predictions(lasso_rs)
```


```{r lasso-plot}
lasso_rs_predictions %>% 
  group_by(id) %>% 
  roc_curve(truth = fatal, .pred_fatal) %>% 
  autoplot() +
  labs(
    color = "Resamples",
    title = "ROC for climbing incident reports"
  )
```

```{r lasso-conf-mat}
conf_mat_resampled(lasso_rs, tidy = F) %>% 
  autoplot(type = "heatmap")

```

Recall that the penalty is a model hyperparameter (lambda). The higher it is, the more model coefficients are reduced (sometimes to 0, removing them -- feature selection). We set it manually before, but we can also estimate its best value, again by training many models on resampled data sets and examining their performance.

```{r penalty-tuning-specification}
tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% #mixture is it tells if you want to use which penalty term you want to input. in this case we are saying (for those that are eligble) that we want it to go to zero (aka remove them)
  set_mode("classification") %>% 
  set_engine("glmnet")

tune_spec

```

```{r lamba}
#try a bunch of numbers as our parameters to see what is the best parameter for our model
lambda_grid <- grid_regular(penalty(), levels = 30)

lambda_grid
```

Here we use grid_regular() to create 30 possible values for the regularization penalty. Then tune_grid() fits a model at each of those values.


```{r tune}
tune_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(tune_spec)

set.seed(2023)
tune_rs  <- tune_grid(
  tune_wf, incidents_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = T)
)
```

```{r plot_metrics}
collect_metrics(tune_rs)


  autoplot(tunes_rs) + 
  labs(
    title = "Lasso performance across reg. penalties"
  )
```

```{r penalty-show-best}
tunes_rs %>% 
  show_best("roc_auc")

tunes_rs %>% 
  show_best("accuracy")

chosen_auc <- tunes_rs %>% 
  select_by_one_std_err(metric = "roc_auc", -penalty) #close enough to the optimal point, but also allows model to drop some points to make it more simple
```

OK, our tuning results have identified the best regularization penalty. Let's finalize our workflow with it. 

```{r final-model}
final_lasso <- finalize_workflow(tune_wf, chosen_auc)

final_lasso
```

The penalty argument value now reflects our tuning result. Now we fit to our training data.

```{r}
fitted_lasso <- fit(final_lasso, incidents_train)

fitted_lasso
```

First let's look at the words associated with an accident being non-fatal.

```{r words-non-fatal}
fitted_lasso %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  arrange(-estimate)
```

And now the words that are most associated with a fatal incident.

```{r words-fatal}
fitted_lasso%>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  arrange(estimate)
```

Finally, let's fit to the test data and see how we did.
```{r}
last_fit(final_lasso, incidents_split) %>% 
  collect_metrics()
```

